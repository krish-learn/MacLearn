---
title: "MachingLearning-Final Project"
author: "KK"
date: "November 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Background
The project objective is to use data from accelerometers (like Jawbone Up, Nike FuelBand, and Fitbit) on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. (for more information visit: http://groupware.les.inf.puc-rio.br/har - see the section on the Weight Lifting Exercise Dataset).

Outcome variable is classe, a factor variable with 5 levels. For this data set, "participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

- exactly according to the specification (Class A)
- throwing the elbows to the front (Class B)
- lifting the dumbbell only halfway (Class C)
- lowering the dumbbell only halfway (Class D)
- throwing the hips to the front (Class E)

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes." [1]

### Objective
1. The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. 
2. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. 
3. You will also use your prediction model to predict 20 different test cases.

### Approach

 - Load the data set and briefly learn the characteristics of the data
 - Use cross-validation method to built a valid model; 70% of the original data is used for model building (training data) while the rest of 30% of the data is used for testing (testing data)
 - Since the number of variables in the training data is too large, clean the data by 1) excluding variables which apparently cannot be explanatory variables, and 2) reducing variables with little information.
 - PCA to reduce the number of variables
 - Two models will be tested using decision tree and random forest algorithms. The model with the highest accuracy will be chosen as our final model.
 - Check the model with the testing data set
 - Apply the model to estimate classes of 20 observations
 
## Solution

### Installing Packages, Loading libraries and setting Seeds

```{r initial}
    library(caret) 
    library(randomForest)
    library(rpart)
    set.seed(1717)
```

### Loading data
```{r data}
    traindata <- read.csv("C:/Users/Raman/Documents/AI/JHU_DataScience/Practical ML/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
    testdata <- read.csv("C:/Users/Raman/Documents/AI/JHU_DataScience/Practical ML/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
    
    dim(traindata)
    dim(testdata)
  
    # Delete columns with all missing values
    traindata<-traindata[,colSums(is.na(traindata)) == 0]
    testdata <-testdata[,colSums(is.na(testdata)) == 0]
    
    # check dimensions of new dataset we created
    dim(traindata)
    dim(testdata)
    
    # Remove variables that are irrelevant for prediction
    traindata   <-traindata[,-c(1:7)]
    testdata <-testdata[,-c(1:7)]

    # and have a look at our new datasets:
    dim(traindata)
    dim(testdata)
```


### Partitioning training data into 2 sets in 70:30 split to allow for cross-validation

```{r partition}

    subsample <- createDataPartition(y=traindata$classe, p=0.7, list=FALSE)
    subtrain <- traindata[subsample, ] 
    subtest <- traindata[-subsample, ]
    dim(subtrain)
    dim(subtest)
 
```

### Model1 - Prediction with Decision Tree

```{r model1}
    # Build model with Training data
    model1 <- rpart(classe ~ ., data=subtrain, method="class")
    
    # Predict with Test data
    prediction1 <- predict(model1, subtest, type = "class")
    
    # Test results
    confusionMatrix(prediction1, subtest$classe)
```

### Model2 - Prediction with Random Forest

```{r model2}

    # Build the model with training data
    model2 <- randomForest(classe ~. , data=subtrain, method="class")

    # Predict with testing data
    prediction2 <- predict(model2, subtest, type = "class")
    
    # Test results
    confusionMatrix(prediction2, subtest$classe)
```

### Accuracy for Random Forest model is 99% compared to Decision Tree model accuracy of 74% and hence we go with Random Forest model to test the sample data given.

### Expected out-of-sample error
Given the accuracy of 99.5% with the Random Forest model, the expected out-of-sample error is 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. The given test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, there should be ZERO Out-of-sample error. 

```{r test}

    predicttest <- predict(model2, testdata, type="class")
    predicttest
    
```

    